name: MLops Weekly Retrain

permissions:
  id-token: write
  contents: read

on:
  # Prefer schedule + manual for retraining
  schedule:
    - cron: '0 13 * * 0'  # Every Sunday 8am EST â†’ 13:00 UTC
  workflow_dispatch:

  # OPTIONAL: only retrain on pushes that affect training pipeline
  push:
    branches: [main]
    paths:
      - "scripts/**"
      - "requirements.txt"
      - ".github/workflows/retrain.yml"
      - "models/**"

env:
  POSTGRES_USER: demo
  POSTGRES_PASSWORD: password
  POSTGRES_DB: demo
  POSTGRES_HOST: localhost
  POSTGRES_PORT: "5432"
  WORKFLOW_TABLE: user_events
  AWS_REGION: us-east-1
  S3_MODEL_URI: s3://thurmans-demo-models/mlops/models/model.pkl

jobs:
  seed-and-train:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U demo"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python 3.13
        uses: actions/setup-python@v4
        with:
          python-version: "3.13"

      - name: Install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt

      - name: Wait for Postgres
        run: |
          until pg_isready -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${POSTGRES_USER}"; do
            echo "Waiting for Postgres..."
            sleep 2
          done

      - name: Seed database tables
        run: |
          source venv/bin/activate
          python scripts/seed_data.py
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Train / Retrain model
        run: |
          source venv/bin/activate
          python scripts/train.py
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Upload trained model artifact
        uses: actions/upload-artifact@v4
        with:
          name: mlops-model
          path: models/**/model.pkl
          if-no-files-found: warn
          compression-level: 6
          overwrite: false

      # Only publish from main branch
      - name: Configure AWS credentials (OIDC)
        if: ${{ github.ref == 'refs/heads/main' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::080967118593:role/thurmans-mlops-demo-role
          aws-region: ${{ env.AWS_REGION }}

      - name: Validate AWS identity
        if: ${{ github.ref == 'refs/heads/main' }}
        run: aws sts get-caller-identity

      - name: Upload model.pkl to S3
        if: ${{ github.ref == 'refs/heads/main' }}
        run: |
          echo "Models found:" && find models -maxdepth 6 -type f -name "model.pkl" -print
          python - <<'PY'
          import os, glob, subprocess
          s3_uri = os.environ["S3_MODEL_URI"]
          candidates = glob.glob("models/**/model.pkl", recursive=True)
          assert candidates, "No model.pkl found under models/**/model.pkl"
          model_path = max(candidates, key=lambda p: os.path.getsize(p))
          size = os.path.getsize(model_path)
          print(f"Uploading: {model_path} ({size} bytes) -> {s3_uri}")
          subprocess.check_call(["aws","s3","cp", model_path, s3_uri])
          subprocess.check_call(["aws","s3","ls", os.path.dirname(s3_uri) + "/"])
          PY
